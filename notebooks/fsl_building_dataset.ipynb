{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IObUwIp6lEzM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install datasets\n",
        "!pip install ultralytics=8.0.227"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPAQy4GklURs",
        "outputId": "5d8a47a5-88f5-4aa9-e07a-58b3167f4a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.0.227\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import Polygon\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import SAM, YOLO\n",
        "\n",
        "print(ultralytics.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RhnpZHJ4L3Gv"
      },
      "outputs": [],
      "source": [
        "def custom_auto_annotate(data, bbx_file, sam_model='sam_b.pt', device='', output_dir='./'):\n",
        "    \"\"\"\n",
        "    Automatically annotates images using a YOLO object detection model and a SAM segmentation model.\n",
        "\n",
        "    Args:\n",
        "        data (str): Path to a folder containing images to be annotated.\n",
        "        bbx_file (str): Path to a file with bounding_boxes annotated to images.\n",
        "        sam_model (str, optional): Pre-trained SAM segmentation model. Defaults to 'sam_b.pt'.\n",
        "        device (str, optional): Device to run the models on. Defaults to an empty string (CPU or GPU, if available).\n",
        "        output_dir (str, optional): Directory to save the annotated results.\n",
        "    \"\"\"\n",
        "    # Load SAM model\n",
        "    sam_model = SAM(sam_model)\n",
        "\n",
        "    # Create directory if doesn't exist\n",
        "    path = Path(output_dir)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(bbx_file, 'r') as bbx_f:\n",
        "      lines = bbx_f.readlines()\n",
        "\n",
        "    image_path = None\n",
        "    bounding_boxes = []\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if not line:\n",
        "          continue # skip empty lines\n",
        "\n",
        "      image_path_match = re.match(r'^([^/]+/[^ ]+\\.jpg)$', line) # Regex for image path\n",
        "      bounding_box_count_match = re.fullmatch(r'\\d+$', line) # Regex for number of bounding boxes\n",
        "      bounding_box_match = re.fullmatch(r'(\\d+) (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) (\\d+) (\\d+)$', line) # Regex for bounding boxes\n",
        "\n",
        "      if image_path_match: # New file so we process the previous data\n",
        "        print(f\"processing image: {image_path_match.group(1)}\")\n",
        "\n",
        "        if bounding_boxes == [] or len(bounding_boxes) > 50: # We don't process images with no bounding boxes or more than 50\n",
        "          image_path = image_path_match.group(1)\n",
        "          continue\n",
        "\n",
        "        # Compute mask with SAM\n",
        "        full_image_path = data / Path(image_path)\n",
        "        sam_results = sam_model(full_image_path, bboxes=np.array(bounding_boxes), verbose=False, save=False, device=device)\n",
        "        segments = sam_results[0].masks.xyn\n",
        "\n",
        "        # Save mask\n",
        "        with open(f'{str(Path(output_dir) / Path(image_path).stem)}.txt', 'w') as f:\n",
        "          for i in range(len(segments)):\n",
        "            s = segments[i]\n",
        "            if len(s) == 0:\n",
        "              continue\n",
        "            segment = map(str, segments[i].reshape(-1).tolist())\n",
        "            f.write(f'{0} ' + ' '.join(segment) + '\\n') # We put 0 because we only consider face\n",
        "\n",
        "        # Next image\n",
        "        image_path = image_path_match.group(1)\n",
        "        bounding_boxes.clear()\n",
        "\n",
        "      elif bounding_box_count_match: # We don't need this information\n",
        "        continue \n",
        "      elif bounding_box_match: # Get current bounding box\n",
        "          box_info = [int(val) for val in bounding_box_match.groups()]\n",
        "          x, y, width, height = 0, 1, 2, 3\n",
        "          box = np.array([\n",
        "                box_info[x],\n",
        "                box_info[y],\n",
        "                box_info[x] + box_info[width],\n",
        "                box_info[y] + box_info[height]\n",
        "            ])\n",
        "          bounding_boxes.append(box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_vMn6VT8MC6t"
      },
      "outputs": [],
      "source": [
        "data = \"../data/images\"\n",
        "bbx_file = '../data/bounding_boxes/wider_faceseglite_bbx.txt'\n",
        "sam_model = 'sam_b.pt'\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "output_directory = f\"../data/masks_{datetime.now()}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqgDWoDLMUaw"
      },
      "outputs": [],
      "source": [
        "custom_auto_annotate(data=data, bbx_file=bbx_file, sam_model=sam_model, device=device, output_dir=output_directory)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
